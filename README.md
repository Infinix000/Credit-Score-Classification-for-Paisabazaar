Credit Score Classification for Paisabazaar📖 OverviewThis project focuses on developing a robust machine learning model to predict customer credit scores for Paisabazaar, a leading financial services platform. By analyzing a rich dataset of customer financial and behavioral data, the model classifies credit scores into three categories: Good, Standard, and Poor.The primary goal is to empower Paisabazaar with a data-driven tool to enhance risk management, streamline loan approvals, and offer personalized financial products to its customers.🎯 Problem StatementPaisabazaar needs an accurate and automated system to assess the creditworthiness of loan applicants. The key business objectives are:Reduce Risk: Minimize loan defaults by accurately identifying high-risk applicants.Improve Efficiency: Automate the credit assessment process for faster and more consistent decision-making.Personalize Services: Segment customers based on their credit profiles to offer tailored financial products and advice.Data-Driven Decisions: Replace or augment traditional credit assessment methods with a predictive model based on historical data.📊 DatasetThe analysis was performed on a dataset containing 100,000 customer records with 27 distinct features. The features encompass a wide range of information, including:Demographics: Age, OccupationFinancial Health: Annual Income, Monthly Balance, Outstanding DebtCredit Behavior: Number of Loans, Payment History, Credit Utilization Ratio, Credit MixTarget Variable: Credit_Score (Categorical: Poor, Standard, Good)⚙️ Project WorkflowThe project followed a structured machine learning pipeline:Data Cleaning & Preprocessing: Handled missing values, corrected data types, and dropped irrelevant columns (ID, Name, SSN).Feature Engineering: Created new insightful features like Debt_to_Income and EMI_to_Income ratios. Transformed the multi-valued Type_of_Loan column into one-hot encoded features.Exploratory Data Analysis (EDA): Generated univariate and bivariate visualizations to uncover key relationships between features and the target credit score.Model Building: Trained and evaluated three powerful gradient-boosting and ensemble models:Random Forest ClassifierXGBoost ClassifierLightGBM ClassifierModel Evaluation: Assessed model performance using Accuracy, F1-Score, Classification Reports, and Confusion Matrices.Feature Importance: Analyzed feature importances from the best model to identify the key drivers of creditworthiness.📈 Modeling and ResultsAll models performed well, but the LightGBM Classifier delivered the best results, demonstrating a superior ability to classify credit scores accurately.| Model | Accuracy | F1-Score (Macro) || Random Forest | 82.1% | 77.5% || XGBoost | 84.8% | 80.9% || LightGBM | 85.2% | 81.4% |The confusion matrix for the LightGBM model shows its effectiveness in distinguishing between the 'Good', 'Standard', and 'Poor' credit score classes.Key Predictors of Credit ScoreThe feature importance analysis confirmed that financial discipline and debt management are the most critical factors. The top predictors were:Interest_RateCredit_MixOutstanding_DebtDebt_to_Income RatioCredit_History_Age🛠️ Tech StackData Manipulation: Pandas, NumPyData Visualization: Matplotlib, SeabornMachine Learning: Scikit-learn, XGBoost, LightGBMDevelopment Environment: Jupyter Notebook, Google Colab🚀 How to RunTo replicate this project on your local machine, follow these steps:Clone the repository:git clone [https://github.com/your-username/your-repo-name.git](https://github.com/your-username/your-repo-name.git)
cd your-repo-name

Install the required dependencies:pip install -r requirements.txt

(Note: You will need to create a requirements.txt file by running pip freeze > requirements.txt in your environment.)Launch Jupyter Notebook:jupyter notebook

Run the notebook: Open the .ipynb file and execute the cells sequentially.🔮 Future WorkHyperparameter Tuning: Use GridSearchCV or Optuna to fine-tune the LightGBM model for potentially higher performance.Model Explainability (XAI): Integrate SHAP to explain individual predictions, providing transparency and building trust in the model's decisions.Deployment: Package the final model into a REST API using Flask or FastAPI and containerize it with Docker for scalable deployment.
